{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Compression in Federated Learning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB4qCry1qA3T",
        "colab_type": "text"
      },
      "source": [
        "In this notebook we sketch the implementation of [Artemis](https://arxiv.org/pdf/2006.14591.pdf). In artemis, every device keeps a memory variable $h_{i}$ to track the gradient in order to insure the convergence, and exchanges the compressed version of the difference between the gradient and this value with the server."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7T6OGZ0Zp8rK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 64\n",
        "N_WORKERS = 10\n",
        "LR_ = 1e-3\n",
        "N_ROUNDS = 100"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbEIsupVqJYd",
        "colab_type": "text"
      },
      "source": [
        "# Compression \n",
        "\n",
        "\n",
        "We first need to implement a compression scheme, in this example we use quantization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "106o_JtSqIHe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torch.distributions.bernoulli import Bernoulli\n",
        "\n",
        "class QuantizationCompressor(object):\n",
        "    \"\"\"Taken from https://github.com/epfml/ChocoSGD/\"\"\"\n",
        "    \n",
        "    def get_qsgd(self, x, s, is_biased=False):\n",
        "        norm = x.norm(p=2)\n",
        "        level_float = s * x.abs() / norm\n",
        "        previous_level = torch.floor(level_float)\n",
        "        is_next_level = (torch.rand_like(x) < (level_float - previous_level)).float()\n",
        "        new_level = previous_level + is_next_level\n",
        "\n",
        "        scale = 1\n",
        "        if is_biased:\n",
        "            d = x.nelement()\n",
        "            scale = 1.0 / (min(d / (s ** 2), math.sqrt(d) / s) + 1.0)\n",
        "        return scale * torch.sign(x) * norm * new_level / s\n",
        "\n",
        "    def qsgd_quantize_numpy(self, x, s, is_biased=False):\n",
        "        \"\"\"quantize the tensor x in d level on the absolute value coef wise\"\"\"\n",
        "        norm = np.sqrt(np.sum(np.square(x)))\n",
        "        level_float = s * np.abs(x) / norm\n",
        "        previous_level = np.floor(level_float)\n",
        "        is_next_level = np.random.rand(*x.shape) < (level_float - previous_level)\n",
        "        new_level = previous_level + is_next_level\n",
        "\n",
        "        scale = 1\n",
        "        if is_biased:\n",
        "            d = len(x)\n",
        "            scale = 1.0 / (np.minimum(d / s ** 2, np.sqrt(d) / s) + 1.0)\n",
        "        return scale * np.sign(x) * norm * new_level / s\n",
        "\n",
        "    def compress(self, x: torch.FloatTensor, s: int) -> torch.FloatTensor:\n",
        "      if s == 0:\n",
        "        return x\n",
        "      norm_x = torch.norm(x, p=2)\n",
        "      if norm_x == 0:\n",
        "          return x\n",
        "      ratio = torch.abs(x) / norm_x\n",
        "      l = torch.floor(ratio * s)\n",
        "      p = ratio * s - l\n",
        "      sampled = Bernoulli(p).sample()\n",
        "      qtzt = torch.sign(x) * norm_x * (l + sampled) / s\n",
        "      return qtzt\n",
        "\n",
        "    def compress_orignal(self, arr, quantize_level=8, is_biased=False):\n",
        "        if quantize_level != 32:\n",
        "            s = 2 ** quantize_level - 1\n",
        "            values = self.get_qsgd(arr, s, is_biased)\n",
        "        else:\n",
        "            values = arr\n",
        "        return values\n",
        "\n",
        "    def uncompress(self, arr):\n",
        "        return arr\n",
        "\n",
        "\n",
        "compressor = QuantizationCompressor()\n"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0zSULOaLqWqY",
        "colab_type": "text"
      },
      "source": [
        "# Dataset\n",
        "\n",
        "We simply use iid MNIST here, can be changed with any dataset later, Note that you only need to specify a list of `torch.utils.data.DataLoaders`, each of them is a loader of the dataset of a given client.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMMUjn20qvxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import DataLoader, Subset\n",
        "from torchvision.datasets import MNIST\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "trans = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Normalize((0.5,), (1.0,))\n",
        "                                 ])\n",
        "\n",
        "# TODO : To test, not sure what it does exactly.\n",
        "dataset = Subset(MNIST(root=\"./\", download=True, transform=trans), range(300))\n",
        "\n",
        "loaders = []\n",
        "for _ in range(N_WORKERS):\n",
        "    loader = DataLoader(dataset, shuffle=True, batch_size=BATCH_SIZE)\n",
        "    loaders.append(loader)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wX39SBLUqQms",
        "colab_type": "text"
      },
      "source": [
        "# Learner\n",
        "\n",
        "We also implement a `Learner` class that will be used to train and evaluate a deep-learning model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hRPhxOfiqObm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class Learner:\n",
        "    \"\"\"\n",
        "    Responsible of training and evaluating a (deep-)learning model\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    model (nn.Module): the model trained by the learner\n",
        "    criterion (torch.nn.modules.loss): loss function used to train the `model`\n",
        "    metric (fn): function to compute the metric, should accept as input two vectors and return a scalar\n",
        "    device (str or torch.device):\n",
        "    optimizer (torch.optim.Optimizer):\n",
        "\n",
        "    Methods\n",
        "    ------\n",
        "    fit_batch: perform an optimizer step over one batch\n",
        "    fit_batches: perform successive optimizer steps over successive batches\n",
        "    evaluate_iterator: evaluate `model` on an iterator\n",
        "    get_param_tensor: get `model` parameters as a unique flattened tensor\n",
        "    set_param_tensor: \n",
        "    get_grad_tensor:\n",
        "    set_grad: \n",
        "    \"\"\"\n",
        "\n",
        "    # TODO: decorate getters and setters\n",
        "    def __init__(self, model, criterion, metric, device, optimizer):\n",
        "        self.model = model.to(device)\n",
        "        self.criterion = criterion.to(device)\n",
        "        self.metric = metric\n",
        "        self.device = device\n",
        "        self.optimizer = optimizer\n",
        "\n",
        "    def compute_batch_gradients(self, iterator):\n",
        "        \"\"\"\n",
        "        perform one forward-backward propagation on one batch drawn from `iterator`\n",
        "        :param iterator:\n",
        "        :type iterator: torch.utils.data.DataLoader\n",
        "        return:\n",
        "            loss.item()\n",
        "            metric.item()\n",
        "        \"\"\"\n",
        "        self.model.train()\n",
        "\n",
        "        x, y = next(iter(iterator))\n",
        "        x = x.to(self.device).type(torch.float32)\n",
        "        y = y.to(self.device).type(torch.int64)\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "\n",
        "        y_pred = self.model(x).squeeze()\n",
        "        loss = self.criterion(y_pred, y)\n",
        "        metric = self.metric(y_pred, y)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        return loss.item(), metric.item()\n",
        "\n",
        "\n",
        "    def optimizer_step(self):\n",
        "        \"\"\"\n",
        "        performs one optimizer step after gradients are computed\n",
        "        return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # TODO: add a flag + assertion to determine if gredients are computed or not\n",
        "        self.optimizer.step()\n",
        "\n",
        "    def fit_batch(self, iterator):\n",
        "        \"\"\"\n",
        "        perform an optimizer step over one batch drawn from `iterator`\n",
        "        :param iterator:\n",
        "        :type iterator: torch.utils.data.DataLoader\n",
        "        return:\n",
        "            loss.item()\n",
        "            metric.item()\n",
        "        \"\"\"\n",
        "        loss, metric = self.compute_batch_gradients(iterator)\n",
        "\n",
        "        self.optimizer_step()\n",
        "\n",
        "        return loss, metric\n",
        "\n",
        "    def evaluate_iterator(self, iterator):\n",
        "        \"\"\"\n",
        "        evaluate learner on `iterator`\n",
        "        :param iterator:\n",
        "        :type iterator: torch.utils.data.DataLoader\n",
        "        :return\n",
        "            global_loss and  global_metric accumulated over the iterator\n",
        "        \"\"\"\n",
        "        self.model.eval()\n",
        "\n",
        "        global_loss = 0\n",
        "        global_metric = 0\n",
        "\n",
        "        for x, y in iterator:\n",
        "            x = x.to(self.device).type(torch.float32)\n",
        "            y = y.to(self.device).type(torch.int64)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                y_pred = self.model(x).squeeze()\n",
        "                global_loss += self.criterion(y_pred, y).item()\n",
        "                global_metric += self.metric(y_pred, y).item()\n",
        "\n",
        "        return global_loss, global_metric\n",
        "\n",
        "    def fit_batches(self, iterator, n_steps):\n",
        "        \"\"\"\n",
        "        perform successive optimizer steps over successive batches drawn from iterator\n",
        "        :param iterator:\n",
        "        :type iterator: torch.utils.data.DataLoader\n",
        "        :param n_steps: number of successive batches\n",
        "        :type n_steps: int\n",
        "        :return:\n",
        "            average loss and metric over the `n_steps`\n",
        "        \"\"\"\n",
        "        global_loss = 0\n",
        "        global_acc = 0\n",
        "\n",
        "        for step in range(n_steps):\n",
        "            batch_loss, batch_acc = self.fit_batch(iterator)\n",
        "            global_loss += batch_loss\n",
        "            global_acc += batch_acc\n",
        "\n",
        "        return global_loss / n_steps, global_acc / n_steps\n",
        "\n",
        "    def get_param_tensor(self):\n",
        "        \"\"\"\n",
        "        get `model` parameters as a unique flattened tensor\n",
        "        :return: torch.tensor\n",
        "        \"\"\"\n",
        "        param_list = []\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            param_list.append(param.data.view(-1, ))\n",
        "\n",
        "        return torch.cat(param_list)\n",
        "\n",
        "    def set_param_tensor(self, x):\n",
        "        # add assertion on the shape of x\n",
        "        idx = 0\n",
        "        for param in self.model.parameters():\n",
        "            shape = param.shape\n",
        "            param.data = x[idx:idx+param.view(-1, ).shape[0]].view(shape)\n",
        "\n",
        "            idx += param.view(-1, ).shape[0]\n",
        "\n",
        "    def get_grad_tensor(self):\n",
        "        \"\"\"\n",
        "        get `model` gradients as a unique flattened tensor\n",
        "        :return: torch.tensor\n",
        "        \"\"\"\n",
        "        grad_list = []\n",
        "\n",
        "        for param in self.model.parameters():\n",
        "            grad_list.append(param.grad.data.view(-1, ))\n",
        "\n",
        "        return torch.cat(grad_list)\n",
        "\n",
        "    def set_grad_tensor(self, x):\n",
        "        \"\"\"\n",
        "        set the gradients from a tensor\n",
        "        :return:\n",
        "            None\n",
        "        \"\"\"\n",
        "        # add assertion on the shape of x\n",
        "        idx = 0\n",
        "        for param in self.model.parameters():\n",
        "            shape = param.shape\n",
        "            param.grad.data = x[idx:idx+param.view(-1, ).shape[0]].view(shape)\n",
        "\n",
        "            idx += param.view(-1, ).shape[0]\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Aur3UpIsOdD",
        "colab_type": "text"
      },
      "source": [
        "# Models\n",
        "\n",
        "We use a two layer neural network, can be replaced with whatever is needed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQIWn9XHsZZm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "class TwoLayersModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(TwoLayersModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(784, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 784)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "def accuracy(y_pred, y):\n",
        "    _, predicted = torch.max(y_pred, 1)\n",
        "    correct = (predicted == y).float()\n",
        "    acc = correct.sum() / len(correct)\n",
        "    return acc\n",
        "\n",
        "\n",
        "def get_optimizer(optimizer_name, model, lr_initial):\n",
        "    \"\"\"\n",
        "    Gets torch.optim.Optimizer given an optimizer name,\n",
        "     a model and learning rate\n",
        "    :param optimizer_name: possible are adam and sgd\n",
        "    :type optimizer_name: str\n",
        "    :param model: model to be optimized\n",
        "    :type optimizer_name: nn.Module\n",
        "    :param lr_initial: initial learning used to build the optimizer\n",
        "    :type lr_initial: float\n",
        "    :return: torch.optim.Optimizer\n",
        "    \"\"\"\n",
        "    \n",
        "    if optimizer_name == \"adam\":\n",
        "        return optim.Adam([param for param in model.parameters() \n",
        "                           if param.requires_grad], lr=lr_initial)\n",
        "\n",
        "    elif optimizer_name == \"sgd\":\n",
        "        return optim.SGD([param for param in model.parameters()\n",
        "                          if param.requires_grad], lr=lr_initial)\n",
        "\n",
        "    else:\n",
        "        raise NotImplementedError(\"Other optimizer are not implemented\")\n",
        "\n",
        "def get_learner(device, optimizer_name, initial_lr, seed=1234):\n",
        "    \"\"\"\n",
        "    constructs the learner corresponding to an experiment for a given seed\n",
        "\n",
        "    :param device: used device; possible `cpu` and `cuda`\n",
        "    :param optimizer_name: passed as argument to get_optimizer\n",
        "    :param initial_lr: initial value of the learning rate\n",
        "    :param seed:\n",
        "    :return: Learner\n",
        "    \"\"\"\n",
        "    torch.manual_seed(seed)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    metric = accuracy\n",
        "    model = TwoLayersModel()\n",
        "\n",
        "    optimizer = get_optimizer(optimizer_name=optimizer_name,\n",
        "                              model=model,\n",
        "                              lr_initial=initial_lr)\n",
        "\n",
        "    return Learner(model=model,\n",
        "                   criterion=criterion,\n",
        "                   metric=metric,\n",
        "                   device=device,\n",
        "                   optimizer=optimizer\n",
        "                   )\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JLX6pwAQtjOw",
        "colab_type": "text"
      },
      "source": [
        "## Artemis\n",
        "\n",
        "\n",
        "This is the core class of this project "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rnN12DmhtnTx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Artemis(object):\n",
        "    \"\"\"\n",
        "    Artemis class responsible of running federated learning with double gradient compression scheme\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    learners (List of Learner): Each entry is responsible of training and evaluating a (deep-)learning model\n",
        "    loaders (List of torch.utils.data.DataLoader): loader for each client dataset\n",
        "    alpha (float): parameter to control memory depth \n",
        "    variant (int): determine which variant of artemis to use\n",
        "\n",
        "    Methods\n",
        "    ------\n",
        "    step: perform an optimizer step over one batch\n",
        "    print_logs: \n",
        "    \"\"\"\n",
        "    \n",
        "    def __init__(self, learners, loaders, device, alpha=0.1, variant=1):\n",
        "\n",
        "        assert len(learners) > 0, \"Make sure learners is not empty\"\n",
        "        assert len(learners) == len(loaders), 'Make sure you have the same number of learners and loaders'\n",
        "        assert variant in [0, 1, 2], \"Variant must be 0 (no compression), 1 (uni-compression) or 2 (bi-compression).\"\n",
        "        assert device in [\"cpu\", \"cuda\"] \"Device must be either 'cpu'; either 'cuda'.\"\n",
        "\n",
        "        self.learners = learners\n",
        "        self.loaders = loaders\n",
        "        self.alpha = alpha\n",
        "        self.variant = variant\n",
        "        self.device = device\n",
        "\n",
        "        self.model_size = learners[0].get_param_tensor().shape[0]\n",
        "        self.n_workers = len(learners)\n",
        "\n",
        "        self.memory_terms = torch.zeros(self.n_workers, self.model_size).to(self.device)\n",
        "        self.global_memory_term = torch.zeros(self.model_size).to(self.device)\n",
        "\n",
        "    def step(self):\n",
        "        # TODO: should be initialized with proper shape in future for efficiency\n",
        "        average_compressed_delta = torch.zeros(self.model_size).to(self.device) # tracks the average compressed delta\n",
        "\n",
        "        # Compute and compress local gradietns\n",
        "        for worker_id, learner in enumerate(learners):\n",
        "            learner.compute_batch_gradients(loaders[worker_id])\n",
        "\n",
        "            delta = learner.get_grad_tensor() - self.memory_terms[worker_id, :]\n",
        "\n",
        "            if self.variant == 0:\n",
        "              compressed_delta = delta\n",
        "            else:\n",
        "              compressed_delta = compressor.compress(delta, s=1)\n",
        "\n",
        "            average_compressed_delta += compressed_delta\n",
        "\n",
        "            # Update memory Term\n",
        "            self.memory_terms[worker_id, :] += self.alpha * compressed_delta\n",
        "\n",
        "        # Update global memory term and global gradient\n",
        "        average_compressed_delta = (1/N_WORKERS) * average_compressed_delta \n",
        "        average_gradient = self.global_memory_term + average_compressed_delta\n",
        "        self.global_memory_term += self.alpha * average_compressed_delta\n",
        "\n",
        "        if self.variant in [0, 1]:\n",
        "          omega = average_gradient\n",
        "        else:\n",
        "          omega = compressor.compress(average_gradient, s=1)\n",
        "\n",
        "        # Gradient update\n",
        "        for worker_id, learner in enumerate(learners):\n",
        "          learner.set_grad_tensor(omega)\n",
        "          learner.optimizer_step()\n",
        "\n",
        "    def print_logs(self):\n",
        "        \"\"\"\n",
        "        print train/test loss, train/test metric for average model and local models\n",
        "        \"\"\"\n",
        "        global_train_loss = 0\n",
        "        global_train_metric = 0\n",
        "\n",
        "        for iterator in self.loaders:\n",
        "            train_loss, train_metric = self.learners[0].evaluate_iterator(iterator)\n",
        "\n",
        "            global_train_loss += train_loss\n",
        "            global_train_metric += train_metric\n",
        "\n",
        "        global_train_metric /= self.n_workers\n",
        "        global_train_loss /= self.n_workers\n",
        "\n",
        "        print(\"Train/Loss\", global_train_loss)\n",
        "        print(\"Train/Metric\", global_train_metric)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AiVoBIsFQoDH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 973
        },
        "outputId": "4e28693e-dc0b-49ed-845c-3516a1835308"
      },
      "source": [
        "%%time\n",
        "learners = [get_learner(device=\"cuda\", optimizer_name=\"sgd\", initial_lr=LR_)\n",
        "             for _ in range(N_WORKERS)]\n",
        "artemis = Artemis(learners, loaders, 'cuda', variant=2)\n",
        "for round_idx in range(N_ROUNDS):\n",
        "    print('Round:{}...'.format(round_idx))\n",
        "    artemis.print_logs()\n",
        "    artemis.step()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Round:0...\n",
            "Train/Loss 11.554882526397705\n",
            "Train/Metric 0.4992897756397724\n",
            "Round:1...\n",
            "Train/Loss 11.554593157768249\n",
            "Train/Metric 0.49857954755425454\n",
            "Round:2...\n",
            "Train/Loss 11.552077698707581\n",
            "Train/Metric 0.4936079569160938\n",
            "Round:3...\n",
            "Train/Loss 11.548051834106445\n",
            "Train/Metric 0.50142045840621\n",
            "Round:4...\n",
            "Train/Loss 11.546336460113526\n",
            "Train/Metric 0.5021306842565536\n",
            "Round:5...\n",
            "Train/Loss 11.5432950258255\n",
            "Train/Metric 0.5028409108519554\n",
            "Round:6...\n",
            "Train/Loss 11.540909242630004\n",
            "Train/Metric 0.5205965928733349\n",
            "Round:7...\n",
            "Train/Loss 11.539944767951965\n",
            "Train/Metric 0.5177556850016117\n",
            "Round:8...\n",
            "Train/Loss 11.542425155639648\n",
            "Train/Metric 0.527698865532875\n",
            "Round:9...\n",
            "Train/Loss 11.538238573074342\n",
            "Train/Metric 0.5348011411726474\n",
            "Round:10...\n",
            "Train/Loss 11.538372731208801\n",
            "Train/Metric 0.5497159108519554\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-8ca976ef9c02>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'learners = [get_learner(device=\"cuda\", optimizer_name=\"sgd\", initial_lr=LR_)\\n             for _ in range(N_WORKERS)]\\nartemis = Artemis(learners, loaders, \\'cuda\\', variant=2)\\nfor round_idx in range(N_ROUNDS):\\n    print(\\'Round:{}...\\'.format(round_idx))\\n    artemis.print_logs()\\n    artemis.step()'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<decorator-gen-60>\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magic.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(f, *a, **k)\u001b[0m\n\u001b[1;32m    186\u001b[0m     \u001b[0;31m# but it's overkill for just that one bit of state.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    187\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mmagic_deco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 188\u001b[0;31m         \u001b[0mcall\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/IPython/core/magics/execution.py\u001b[0m in \u001b[0;36mtime\u001b[0;34m(self, line, cell, local_ns)\u001b[0m\n\u001b[1;32m   1191\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1193\u001b[0;31m             \u001b[0mexec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlocal_ns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1194\u001b[0m             \u001b[0mend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclock2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-f6b1cde62ae5>\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;31m# Compute and compress local gradietns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearner\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlearners\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m             \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_batch_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloaders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m             \u001b[0mdelta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_grad_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemory_terms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mworker_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-2834cb1296c8>\u001b[0m in \u001b[0;36mcompute_batch_gradients\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 403\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    404\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 257\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindices\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     88\u001b[0m             \u001b[0mtuple\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0mwhere\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mindex\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32mclass\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \"\"\"\n\u001b[0;32m---> 90\u001b[0;31m         \u001b[0mimg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtargets\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     92\u001b[0m         \u001b[0;31m# doing this so that it is consistent with all other datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVNXPtHPy40Q",
        "colab_type": "text"
      },
      "source": [
        "# TODO tasks\n",
        "\n",
        "* Complete the #TODO tasks left inside the code\n",
        "* Use only one learner instead of a list of learners for memory efficiency\n",
        "* Check if the compression class is working (I copied it directly from choco-SGD repo without any tests)\n",
        "* Test the evolution of the parameters on a \"toy example\", for example linear regression."
      ]
    }
  ]
}