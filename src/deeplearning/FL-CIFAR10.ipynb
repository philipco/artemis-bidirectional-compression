{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://towardsdatascience.com/preserving-data-privacy-in-deep-learning-part-1-a04894f78029"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.insert(0, '../..')\n",
    "\n",
    "import syft as sy\n",
    "from src.models.QuantizationModel import s_quantization\n",
    "\n",
    "from src.deeplearning.Optimizer import DianaOptimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################\n",
    "##### importing libraries #####\n",
    "###############################\n",
    "\n",
    "import os\n",
    "import copy\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data.dataset import Dataset   \n",
    "torch.backends.cudnn.benchmark=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### Hyperparameters for federated learning #########\n",
    "num_clients = 10\n",
    "num_selected = num_clients\n",
    "num_rounds = 21\n",
    "epochs = 1\n",
    "batch_size = 64\n",
    "quantization_param = 1\n",
    "lr = 1e-3\n",
    "dl_net = \"VGGKo\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#############################################################\n",
    "##### Creating desired data distribution among clients  #####\n",
    "#############################################################\n",
    "\n",
    "# Image augmentation \n",
    "transform_train = transforms.Compose([\n",
    "    transforms.RandomCrop(32, padding=4),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Loading CIFAR10 using torchvision.datasets\n",
    "traindata = datasets.CIFAR10('./data', train=True, download=True,\n",
    "                       transform= transform_train)\n",
    "\n",
    "# Dividing the training data into num_clients, with each client having equal number of images\n",
    "traindata_split = torch.utils.data.random_split(traindata, [int(traindata.data.shape[0] / num_clients) for _ in range(num_clients)])\n",
    "\n",
    "# Creating a pytorch loader for a Deep Learning model\n",
    "train_loader = [torch.utils.data.DataLoader(x, batch_size=batch_size, shuffle=True) for x in traindata_split]\n",
    "\n",
    "# Normalizing the test images\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010)),\n",
    "])\n",
    "\n",
    "# Loading the test iamges and thus converting them into a test_loader\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "        datasets.CIFAR10('./data', train=False, transform=transforms.Compose([transforms.ToTensor(),transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))])\n",
    "        ), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision.datasets import MNIST\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "trans = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (1.0,))\n",
    "                                 ])\n",
    "\n",
    "# TODO : To test, not sure what it does exactly.\n",
    "dataset = Subset(MNIST(root=\"./\", download=True, train=True, transform=trans), range(300))\n",
    "\n",
    "# TODO : To test, not sure what it does exactly.\n",
    "test_loader = DataLoader(MNIST(root=\"./\", download=True, train=False, transform=trans))\n",
    "\n",
    "loaders = []\n",
    "for _ in range(num_clients):\n",
    "    loader = DataLoader(dataset, shuffle=True, batch_size=batch_size)\n",
    "    loaders.append(loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### Neural Network model #####\n",
    "#################################\n",
    "\n",
    "cfg = {\n",
    "    'VGGKo' : [64, 'M', 128, 'M', 256, 'M', 512, 'M', 512, 'M'],\n",
    "    'VGG11': [64, 'M', 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG13': [64, 64, 'M', 128, 128, 'M', 256, 256, 'M', 512, 512, 'M', 512, 512, 'M'],\n",
    "    'VGG16': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'M', 512, 512, 512, 'M', 512, 512, 512, 'M'],\n",
    "    'VGG19': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 256, 'M', 512, 512, 512, 512, 'M', 512, 512, 512, 512, 'M'],\n",
    "}\n",
    "\n",
    "class VGG(nn.Module):\n",
    "    def __init__(self, vgg_name):\n",
    "        super(VGG, self).__init__()\n",
    "        self.features = self._make_layers(cfg[vgg_name])\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.features(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.classifier(out)\n",
    "        output = F.log_softmax(out, dim=1)\n",
    "        return output\n",
    "\n",
    "    def _make_layers(self, cfg):\n",
    "        layers = []\n",
    "        in_channels = 3\n",
    "        for x in cfg:\n",
    "            if x == 'M':\n",
    "                layers += [nn.MaxPool2d(kernel_size=2, stride=2)]\n",
    "            else:\n",
    "                layers += [nn.Conv2d(in_channels, x, kernel_size=3, padding=1),\n",
    "                           nn.BatchNorm2d(x),\n",
    "                           nn.ReLU(inplace=True)]\n",
    "                in_channels = x\n",
    "        layers += [nn.AvgPool2d(kernel_size=1, stride=1)]\n",
    "        return nn.Sequential(*layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "class TwoLayersModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TwoLayersModel, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 784)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        return self.fc2(x)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### Defensive programming #####\n",
    "#################################\n",
    "\n",
    "def check_model_has_been_updated(old_model, new_model, state):\n",
    "    updated = False\n",
    "    cpt = 0\n",
    "    for old_param, new_param in zip(old_model.parameters(), new_model.parameters()):\n",
    "        cpt += 1\n",
    "        if not torch.equal(old_param, new_param):\n",
    "            updated = True\n",
    "    assert updated == True, \"The %s model hasn't been updated.\" %state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################\n",
    "##### Central server model #####\n",
    "#################################\n",
    "\n",
    "class CentralServer:\n",
    "    \n",
    "    def __init__(self, test_loader):\n",
    "        self.test_loader = test_loader\n",
    "        self.model =  TwoLayersModel()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        \n",
    "    def server_aggregate(self, aggregated_grad):\n",
    "        \"\"\"\n",
    "        This function has aggregation method 'mean'\n",
    "        \"\"\"\n",
    "        ### This will take simple mean of the weights of models ###\n",
    "        old_model = copy.deepcopy(self.model)\n",
    "\n",
    "        for cpt, p in enumerate(self.model.parameters()):\n",
    "            p.grad = aggregated_grad[cpt]\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Checking that the parameters has been updated.\n",
    "        check_model_has_been_updated(old_model, self.model, \"global\")\n",
    "        \n",
    "    def test(self):\n",
    "        \"\"\"This function test the global model on test data and returns test loss and test accuracy \"\"\"\n",
    "        self.model.eval()\n",
    "        test_loss = 0\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in self.test_loader:\n",
    "                data, target = data, target\n",
    "                output = self.model(data)\n",
    "                test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "                pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "                correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "        test_loss /= len(self.test_loader.dataset)\n",
    "        acc = correct / len(self.test_loader.dataset)\n",
    "\n",
    "        return test_loss, acc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Worker:\n",
    "    \n",
    "    def __init__(self, train_loader, ID: int):\n",
    "        self.ID = ID\n",
    "        self.train_loader = train_loader\n",
    "        self.model = TwoLayersModel()\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=lr)\n",
    "        self.criterion = nn.CrossEntropyLoss()\n",
    "        \n",
    "    def client_update(self, local_epoch=1):\n",
    "        \"\"\"\n",
    "        This function updates/trains client model on client data\n",
    "        \"\"\"\n",
    "        self.model.train() # Sets the module in training mode.\n",
    "\n",
    "        local_grad = []\n",
    "\n",
    "        for e in range(local_epoch):\n",
    "            # We handle a single batch.\n",
    "            data, target = next(iter(self.train_loader))\n",
    "            #print(\"ID \" + str(self.ID) + \": \" + str(target))\n",
    "            \n",
    "            y_pred = self.model(data).squeeze()\n",
    "            loss = self.criterion(y_pred, target)\n",
    "            \n",
    "    \n",
    "            loss.backward()\n",
    "            \n",
    "            \n",
    "            \n",
    "            # Local compression\n",
    "            for p in self.model.parameters():\n",
    "                local_grad.append(p.grad.data)#s_quantization(p.grad, quantization_param))\n",
    "                \n",
    "            # We do not carry out the optimization step which update models. \n",
    "            # The model will be updated with the compressed aggregation of all gradients.\n",
    "            #optimizer.step()\n",
    "\n",
    "        return loss, local_grad\n",
    "    \n",
    "    def set_grad_tensor(self, aggregated_grad):\n",
    "        #old_model = copy.deepcopy(self.model)\n",
    "                \n",
    "        for cpt, p in enumerate(self.model.parameters()):\n",
    "            p.grad.data = aggregated_grad[cpt]\n",
    "\n",
    "        \n",
    "        # Checking that the parameters has been updated.\n",
    "        #check_model_has_been_updated(old_model, self.model, \"local\")\n",
    "        \n",
    "    def optimizer_step(self):\n",
    "        self.optimizer.step()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "###### Defining all the workers #########\n",
    "workers = [Worker(loaders[i], i) for i in range(num_clients)]\n",
    "central_server = CentralServer(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==--> averaged loss : tensor(2.3079, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(2.3055, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(535.2595, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(2.8539e+11, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(1.6131e+20, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(1.0203e+29, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(inf, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "10-th round\n",
      "average train loss nan | test loss nan | test acc: 0.098\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n",
      "20-th round\n",
      "average train loss nan | test loss nan | test acc: 0.098\n",
      "==--> averaged loss : tensor(nan, grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "###### List containing info about learning #########\n",
    "losses_train = []\n",
    "losses_test = []\n",
    "acc_train = []\n",
    "acc_test = []\n",
    "# Runnining FL\n",
    "\n",
    "for r in range(1, num_rounds + 1):\n",
    "    # select random clients\n",
    "    client_idx = np.random.permutation(num_clients)[:num_selected]\n",
    "    # client update\n",
    "    loss = 0\n",
    "    grad = []\n",
    "    \n",
    "    # Taking into account partial participation\n",
    "    for i in range(num_selected):\n",
    "        local_loss, local_grad = workers[client_idx[i]].client_update(local_epoch=epochs)\n",
    "        loss += local_loss\n",
    "        if not grad:\n",
    "            grad = local_grad\n",
    "        else:\n",
    "            for i in range(len(grad)):\n",
    "                grad[i] += local_grad[i]\n",
    "                \n",
    "    # Bidirectional compression:\n",
    "    omega = []\n",
    "    for i in range(len(grad)):\n",
    "        omega.append(grad[i] / num_selected)#s_quantization(grad[i] / num_selected, quantization_param))\n",
    "        \n",
    "    print(\"==--> averaged loss :\", loss / num_selected)\n",
    "    \n",
    "    losses_train.append(loss)\n",
    "    # server aggregate\n",
    "    central_server.server_aggregate(omega)\n",
    "    for worker in workers:\n",
    "        worker.set_grad_tensor(omega)\n",
    "        worker.optimizer_step()\n",
    "   \n",
    "    if r % 10 == 0:\n",
    "        test_loss, acc = central_server.test()\n",
    "        losses_test.append(test_loss)\n",
    "        acc_test.append(acc)\n",
    "        print('%d-th round' % r)\n",
    "        print('average train loss %0.3g | test loss %0.3g | test acc: %0.3f' % (loss / num_selected, test_loss, acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan,\n",
      "        nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan, nan],\n",
      "       grad_fn=<SelectBackward>)\n"
     ]
    }
   ],
   "source": [
    "for cpt, p in enumerate(central_server.model.parameters()):\n",
    "    if cpt == 0:\n",
    "        print(p[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
